{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3184,
     "status": "ok",
     "timestamp": 1619548868055,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -120
    },
    "id": "UXJGUSyhTOBg"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import IPython\n",
    "from IPython import display\n",
    "from IPython.display import Javascript, Image\n",
    "\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "from google.colab.output import eval_js\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1619548869790,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -120
    },
    "id": "qXN0rPf1UwbP",
    "outputId": "b3b76069-3164-4e7e-f403-844a80c17787"
   },
   "outputs": [],
   "source": [
    "# Wir machen google-drive dem Notebook bekannt, um Zugriff auf die Daten zu erhalten\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_dir = '/content/drive/MyDrive/hackerschool/module_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1619548872614,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -120
    },
    "id": "K-LfIttMTdbi"
   },
   "outputs": [],
   "source": [
    "def Detect(webcam_frame):\n",
    "\n",
    "    # Der aktuelle Frame der WebCam wird aus dem Parameter gelesen und in ein Numpy-Array übertragen \n",
    "    binary = base64.b64decode(webcam_frame.split(',')[1])            \n",
    "    frame = cv.imdecode(np.frombuffer(binary, np.uint8), -1)\n",
    "    \n",
    "    # Da in Opencv mit BGR-Bilder gearbeitet wird, muss das Bild nach RGB übertragen werden\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "    # Hier wird die Kaskade zur Erkennung von frontalen Gesichtern geladen \n",
    "    face_cascade = cv.CascadeClassifier()\n",
    "    face_cascade.load(os.path.join(base_dir,'data/haarcascade_frontalface_alt.xml'))\n",
    "    \n",
    "    # Hier wird die Kaskade zur Erkennung von Augen geladen \n",
    "    eye_cascade = cv.CascadeClassifier()\n",
    "    eye_cascade.load(os.path.join(base_dir, 'data/haarcascade_eye.xml'))\n",
    "    \n",
    "    # Hier wird die Kaskade zur Erkennung von Mündern geladen \n",
    "    # Diesen Teil sollt ihr gleich selbst implementieren\n",
    "    # Die Datei die ihr braucht heißt: data/haarcascade_smile.xml\n",
    "\n",
    "    # Das RGB-Bild muss für den Algorithmus noch zu einem Schwarz/Weiß-Bild konvertiert werden\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n",
    "    frame_gray = cv.equalizeHist(frame_gray)\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(frame_gray)\n",
    "\n",
    "    # Es wird über die Koordinaten von den erkannten Gesichtern iteriert\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Hier wird ein Rechteck in Grün um das Gesicht gezeichnet\n",
    "        cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255,0), 2)\n",
    "\n",
    "        # Es wird eine Region of Interest (roi) im grauen und im bunten Bild gesetzt\n",
    "        # Damit wird der zu betrachtene Ausschnitt auf das Gesicht reduziert, denn in \n",
    "        # diesem Bereich vermuten wir weitere Bestandteile des Gesichts. \n",
    "        roi_gray = frame_gray[y:y+h, x:x+w]        \n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Im grauen ROI wird nach den Augen gesucht\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        \n",
    "        # Die Augen werden auf dem bunten Bild mit einem Rechteck umrahmt. \n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0, 0,255), 2)\n",
    "\n",
    "        # Fügt ab hier den Source-Code dazu, um auch den Mund im Gesicht zu\n",
    "        # erkennen und diesen mit einem Rechteck zu umrahmen.\n",
    "        # Schaut euch dazu nochmal an, wie die Augen erkannt wurden. \n",
    "        \n",
    "    return compress_to_bytes(frame)\n",
    "\n",
    "output.register_callback('notebook.Detect', Detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1619548875875,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -120
    },
    "id": "Wk7dFWgwTjSz"
   },
   "outputs": [],
   "source": [
    "def compress_to_bytes(data, fmt='png'):\n",
    "    \"\"\"\n",
    "    In dieser Hilfsfunktion wird das Bild in Bytes tranformiert und anschließend\n",
    "    als in ein JSON-Objekt übertragen, damit das Bild and die JavaScript-Funktion \n",
    "    zurück gesendet werden kann\n",
    "    \"\"\"\n",
    "    buff = io.BytesIO()\n",
    "    img = PIL.Image.fromarray(data)    \n",
    "    img.save(buff, format=fmt)\n",
    "    \n",
    "    data_encode = base64.b64encode(buff.getvalue())\n",
    "    \n",
    "    return IPython.display.JSON({'result': data_encode.decode(encoding='utf-8')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1765,
     "status": "ok",
     "timestamp": 1619548882317,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -120
    },
    "id": "gMx8ufWATkXH"
   },
   "outputs": [],
   "source": [
    "def start_webcam():\n",
    "\n",
    "  js = Javascript('''\n",
    "      (async function() {\n",
    "\n",
    "            // Wir erstellen Html-Elemente, welche den Input der WebCam zeigen\n",
    "            var video = document.createElement('video');        \n",
    "            video.id = 'video_stream'\n",
    "            video.style.display = 'hidden';\n",
    "            \n",
    "            var div = document.createElement('div');\n",
    "            div.id = 'video_container';   \n",
    "\n",
    "            // Mit diesem Button können wir die WebCam wieder ausschalten\n",
    "            stop_button = document.createElement('button');\n",
    "            stop_button.id = 'stop_button';\n",
    "            stop_button.innerHTML = 'Stop Video';     \n",
    "\n",
    "            div.appendChild(video);\n",
    "            div.appendChild(stop_button);\n",
    "                          \n",
    "            document.body.appendChild(div);\n",
    "\n",
    "            // Es wird die WebCam abgerufen, damit diese benutzt werden kann\n",
    "            const stream = await navigator.mediaDevices.getUserMedia({video: true});      \n",
    "            video.srcObject = stream;\n",
    "\n",
    "            // Es wir darauf gewartet, dass die WebCam einsatzbereit ist\n",
    "            await video.play(); \n",
    "            \n",
    "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "            \n",
    "            // Das Video der WebCam wird auf eine Canvas gezeichnet\n",
    "            var canvas = document.createElement('canvas');\n",
    "            canvas.id = 'video_canvas';\n",
    "            canvas.width = video.videoWidth;\n",
    "            canvas.height = video.videoHeight;\n",
    "\n",
    "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "            \n",
    "            // Jede Sekunde wird die Detect-Methode aufgerufen, um das Gesicht zu erkennen.\n",
    "            let triggerID = setInterval(async function call_detect() {\n",
    "                \n",
    "                canvas.getContext('2d').drawImage(video, 0, 0);             \n",
    "                content = canvas.toDataURL('image/jpeg', 0.8);\n",
    "\n",
    "                const result = await google.colab.kernel.invokeFunction(\n",
    "                    'notebook.Detect', // Name der Methode.\n",
    "                    [content], // Der aktuelle Frame der WebCam ist der Parameter für die Detect-Methode.\n",
    "                    {}); // kwargs\n",
    "\n",
    "                // Das Resultat der Detect-Methode wird wieder ausgelesen\n",
    "                img_url = result.data['application/json']['result'];\n",
    "                \n",
    "                // Um das Ergebnis der Detect-Methode anzuzeigen, erstellen wir ein neues Image\n",
    "                detected_image = new Image();\n",
    "                detected_image.onload = function () {{\n",
    "\n",
    "                    var image_canvas = document.getElementById('image_canvas');\n",
    "\n",
    "                    if (image_canvas == null) {                          \n",
    "                        image_canvas = document.createElement('canvas');\n",
    "                        image_canvas.id = 'image_canvas';                  \n",
    "                        document.body.appendChild(image_canvas)\n",
    "                    }      \n",
    "                    image_canvas.width = detected_image.width;\n",
    "                    image_canvas.style.width = detected_image.width + 'px';\n",
    "\n",
    "                    image_canvas.height = detected_image.height;\n",
    "                    image_canvas.style.height = detected_image.height + 'px';\n",
    "                    \n",
    "                    image_canvas.style.display = 'block';\n",
    "                    \n",
    "                    // das Bild wird auf die neue Canvas gezeichnet\n",
    "                    image_canvas.getContext('2d').drawImage(detected_image, 0, 0);\n",
    "\n",
    "                  }};\n",
    "\n",
    "                  // Das Resultat der Detect-Methode ist das Bild mit erkannten Gesicht im Byte-Format.\n",
    "                  // Dieses Byte-Array wird als Quelle für das Bild angegeben, damit es dargestellt werden kann.\n",
    "                  detected_image.src = 'data:image/png;charset=utf-8;base64,'+img_url;\n",
    "              \n",
    "              } , 1000);\n",
    "\n",
    "            // Sobald der Button geklickt wird, wird die WebCam ausgeschaltet und die Detect-Methode\n",
    "            // nicht mehr aufgerufen. \n",
    "            stop_button.onclick = function() {                  \n",
    "                  stream.getVideoTracks()[0].stop();\n",
    "                  clearTimeout(triggerID);\n",
    "          };\n",
    "\n",
    "      })();\n",
    "    ''')\n",
    "\n",
    "  display.display(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 985
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1619548885991,
     "user": {
      "displayName": "Lena Herrmann",
      "photoUrl": "",
      "userId": "14614092248171651689"
     },
     "user_tz": -120
    },
    "id": "mU0SZGkIToer",
    "outputId": "c4352bd6-a263-4c39-d4e8-8c94e6893b07"
   },
   "outputs": [],
   "source": [
    "# Mit diesem Methoden-Aufruf starten wir die WebCam.\n",
    "start_webcam()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMGlInvlyY/nXAPHAlqDuOS",
   "collapsed_sections": [],
   "mount_file_id": "15ibQZnin_hcMW7e6pTVhdpq0_Wgs-TlY",
   "name": "module_1b_eye_and_mouth_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
