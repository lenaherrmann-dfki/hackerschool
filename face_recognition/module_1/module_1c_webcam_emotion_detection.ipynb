{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_webcam_emotion_detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHxNnHdkllD6nXi07ECh0h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zH05UOEEQUYV"},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Flatten\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import numpy as np\n","from google.colab import output\n","import cv2 as cv\n","\n","import IPython\n","from IPython import display\n","from IPython.display import Javascript, Image\n","\n","import PIL\n","import os\n","\n","from google.colab.output import eval_js\n","import base64\n","import io\n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DUa4sgbwSLmB"},"source":["# Wir machen google-drive dem Notebook bekannt, um Zugriff auf die Daten zu erhalten\n","from google.colab import drive\n","drive.mount('/content/drive')\n","base_dir = '/content/drive/MyDrive/hackerschool'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBMFK4xNQvmC"},"source":["# Erst wird die Struktur des Models erstellt\n","emotion_detector = Sequential()\n","\n","emotion_detector.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n","emotion_detector.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n","emotion_detector.add(MaxPooling2D(pool_size=(2, 2)))\n","emotion_detector.add(Dropout(0.25))\n","\n","emotion_detector.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","emotion_detector.add(MaxPooling2D(pool_size=(2, 2)))\n","emotion_detector.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n","emotion_detector.add(MaxPooling2D(pool_size=(2, 2)))\n","emotion_detector.add(Dropout(0.25))\n","\n","emotion_detector.add(Flatten())\n","emotion_detector.add(Dense(1024, activation='relu'))\n","emotion_detector.add(Dropout(0.5))\n","emotion_detector.add(Dense(7, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtMp3d5BQ_AW"},"source":["# Jetzt müssen wir die Gewichte vom Model laden, damit wir es tatsächlich verwenden können\n","emotion_detector.load_weights('emotion-model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRWVevxSVkaR"},"source":["# Baut einen Emotion-Detector, der das Model und den Quellcode von gestern verwendet, um Gesichter zu erkennen und die gefundenen Emotionen auf das Bild schreibt\n","\n","# Tipps:\n","# Um das Modell aufzurufen müssten ihr den richtigen Bereich aus dem Bild finden und dem Model übergeben.\n","# Bevor ihr das Model aufrufen könnt, müsst ihr das Bild in die richtige Struktur bringen\n","# \n","# Erst verkleinern\n","#\n","# roi_gray = cv.resize(roi_gray, (48,48))\n","#\n","# und dann die Form ändern\n","#\n","# image = np.expand_dims(np.expand_dims(roi_gray, axis=-1), axis=0)\n","# \n","# Das Bild muss grau sein !!!! Und auf die Größe 48x48 reduziert werden !!!\n","#\n","# Das Modell könnt ihr wie folgt verwenden:\n","#\n","# emotion = emotion_detector.predict(image)\n","#\n","# Verwendet\n","#\n","# emotion = np.argmax(emotion)\n","#\n","# So könnt ihr Text auf das Bild schreiben \n","#\n","# cv.putText(frame, emotion, (x, y), cv.FONT_HERSHEY_SIMPLEX, 0.9, (255,0,0), 5, cv.LINE_AA) \n","#\n","# um die gefundenen Emotion herauszufinden\n","# Es gibt\n","# 0 = Angst\n","# 1 = Eckel\n","# 2 = Froh\n","# 3 = Neutral\n","# 4 = Traurig\n","# 5 = Überrascht\n","# 6 = Wütend \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e979yEx2RaWD"},"source":["def Detect_Emotion(web_frame):\n","  \n","  # Der aktuelle Frame der WebCam wird aus dem Parameter gelesen und in ein Numpy-Array übertragen \n","  binary = base64.b64decode(webcam_frame.split(',')[1])            \n","  frame = cv.imdecode(np.frombuffer(binary, np.uint8), -1)\n","    \n","  # Da in Opencv mit BGR-Bilder gearbeitet wird, muss das Bild nach RGB übertragen werden\n","  frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n","\n","  # Hier wird die Kaskade zur Erkennung von frontalen Gesichtern geladen \n","  face_cascade = cv.CascadeClassifier()\n","  face_cascade.load(os.path.join(base_dir,'data/haarcascade_frontalface_alt.xml'))\n","    \n","  # Das RGB-Bild muss für den Algorithmus noch zu einem Schwarz/Weiß-Bild konvertiert werden\n","  frame_gray = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n","  frame_gray = cv.equalizeHist(frame_gray)\n","    \n","  # Es wird nach Gesichtern in dem grauen Bild gesucht\n","  faces = face_cascade.detectMultiScale(frame_gray)\n","\n","  # Es wird über die Koordinaten von den erkannten Gesichtern iteriert\n","  for (x,y,w,h) in faces:\n","    # Hier wird ein Rechteck in Grün um das Gesicht gezeichnet\n","    cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255,0), 2)\n","    # Es wird eine Region of Interest (roi) im grauen und im bunten Bild gesetzt\n","    # Damit wird der zu betrachtene Ausschnitt auf das Gesicht reduziert, denn in \n","    # diesem Bereich vermuten wir weitere Bestandteile des Gesichts. \n","    roi_gray = frame_gray[y:y+h, x:x+w]\n","    \n","    # ruft die Methode find_emotion auf, um die aktuelle Emotion zu finden\n","\n","    # schreibt die gefundene Emotion auf das Bild \n","        \n","  return compress_to_bytes(frame)\n","\n","output.register_callback('notebook.Detect_Emotion', Detect_Emotion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiEIqc6_Y4f1"},"source":["# schreibt eine Methode, die das Modell aufruft und euch die aktuelle Emotion zurück gibt\n","def find_emotion(roi_gray):\n","  global emotion_detector\n","  \n","  # verkleinert den Bildausschnitt auf 48x48\n","  \n","  # ruft das Modell mit dem aktuellen Bildausschnitt auf\n","\n","  # ermittelt welche Emotion das Model erraten hat\n","\n","  # ruft die Funktion emotion_to_text auf, die die Emotionsklasse in Text übersetzt\n","\n","  # gebt die gefundene Emotion als Rückgabewert zurück"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGdkHt4WZvLv"},"source":["def emotion_to_text(emotion_class):\n","  # überführt die Emotionsklasse in text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRZ-OmB5SQCg"},"source":["def compress_to_bytes(data, fmt='png'):\n","    \"\"\"\n","    In dieser Hilfsfunktion wird das Bild in Bytes tranformiert und anschließend\n","    als in ein JSON-Objekt übertragen, damit das Bild and die JavaScript-Funktion \n","    zurück gesendet werden kann\n","    \"\"\"\n","    buff = io.BytesIO()\n","    img = PIL.Image.fromarray(data)    \n","    img.save(buff, format=fmt)\n","    \n","    data_encode = base64.b64encode(buff.getvalue())\n","    \n","    return IPython.display.JSON({'result': data_encode.decode(encoding='utf-8')})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xj1su9cdSXAe"},"source":["def start_webcam():\n","\n","  js = Javascript('''\n","      (async function() {\n","\n","            // Wir erstellen Html-Elemente, welche den Input der WebCam zeigen\n","            var video = document.createElement('video');        \n","            video.id = 'video_stream'\n","            video.style.display = 'hidden';\n","            \n","            var div = document.createElement('div');\n","            div.id = 'video_container';   \n","\n","            // Mit diesem Button können wir die WebCam wieder ausschalten\n","            stop_button = document.createElement('button');\n","            stop_button.id = 'stop_button';\n","            stop_button.innerHTML = 'Stop Video';     \n","\n","            div.appendChild(video);\n","            div.appendChild(stop_button);\n","                          \n","            document.body.appendChild(div);\n","\n","            // Es wird die WebCam abgerufen, damit diese benutzt werden kann\n","            const stream = await navigator.mediaDevices.getUserMedia({video: true});      \n","            video.srcObject = stream;\n","\n","            // Es wir darauf gewartet, dass die WebCam einsatzbereit ist\n","            await video.play(); \n","            \n","            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","            \n","            // Das Video der WebCam wird auf eine Canvas gezeichnet\n","            var canvas = document.createElement('canvas');\n","            canvas.id = 'video_canvas';\n","            canvas.width = video.videoWidth;\n","            canvas.height = video.videoHeight;\n","\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            \n","            // Jede Sekunde wird die Detect-Methode aufgerufen, um das Gesicht zu erkennen.\n","            let triggerID = setInterval(async function call_detect() {\n","                \n","                canvas.getContext('2d').drawImage(video, 0, 0);             \n","                content = canvas.toDataURL('image/jpeg', 0.8);\n","\n","                const result = await google.colab.kernel.invokeFunction(\n","                    'notebook.Detect', // Name der Methode.\n","                    [content], // Der aktuelle Frame der WebCam ist der Parameter für die Detect-Methode.\n","                    {}); // kwargs\n","\n","                // Das Resultat der Detect-Methode wird wieder ausgelesen\n","                img_url = result.data['application/json']['result'];\n","                \n","                // Um das Ergebnis der Detect-Methode anzuzeigen, erstellen wir ein neues Image\n","                detected_image = new Image();\n","                detected_image.onload = function () {{\n","\n","                    var image_canvas = document.getElementById('image_canvas');\n","\n","                    if (image_canvas == null) {                          \n","                        image_canvas = document.createElement('canvas');\n","                        image_canvas.id = 'image_canvas';                  \n","                        document.body.appendChild(image_canvas)\n","                    }      \n","                    image_canvas.width = detected_image.width;\n","                    image_canvas.style.width = detected_image.width + 'px';\n","\n","                    image_canvas.height = detected_image.height;\n","                    image_canvas.style.height = detected_image.height + 'px';\n","                    \n","                    image_canvas.style.display = 'block';\n","                    \n","                    // das Bild wird auf die neue Canvas gezeichnet\n","                    image_canvas.getContext('2d').drawImage(detected_image, 0, 0);\n","\n","                  }};\n","\n","                  // Das Resultat der Detect-Methode ist das Bild mit erkanntem Gesicht im Byte-Format.\n","                  // Dieses Byte-Array wird als Quelle für das Bild angegeben, damit es dargestellt werden kann.\n","                  detected_image.src = 'data:image/png;charset=utf-8;base64,'+img_url;\n","              \n","              } , 1000);\n","\n","            // Sobald der Button geklickt wird, wird die WebCam ausgeschaltet und die Detect-Methode\n","            // nicht mehr aufgerufen. \n","            stop_button.onclick = function() {                  \n","                  stream.getVideoTracks()[0].stop();\n","                  clearTimeout(triggerID);\n","          };\n","\n","      })();\n","    ''')\n","\n","  display.display(js)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iP5DMyLKSaUG"},"source":["# Mit diesem Methoden-Aufruf starten wir die WebCam.\n","start_webcam()"],"execution_count":null,"outputs":[]}]}