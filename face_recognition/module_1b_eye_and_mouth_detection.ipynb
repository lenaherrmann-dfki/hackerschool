{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"module_1b_eye_and_mouth_detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNsD6zvb3RF02t97fNz+5mz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"UXJGUSyhTOBg","executionInfo":{"status":"ok","timestamp":1615152189980,"user_tz":-60,"elapsed":575,"user":{"displayName":"Lena Herrmann","photoUrl":"","userId":"14614092248171651689"}}},"source":["from google.colab import output\n","import cv2 as cv\n","import numpy as np\n","\n","import IPython\n","from IPython import display\n","from IPython.display import Javascript, Image\n","\n","import PIL\n","import os\n","\n","from google.colab.output import eval_js\n","import base64\n","import io"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXN0rPf1UwbP","executionInfo":{"status":"ok","timestamp":1615152192897,"user_tz":-60,"elapsed":649,"user":{"displayName":"Lena Herrmann","photoUrl":"","userId":"14614092248171651689"}},"outputId":"e5e51aa9-74dc-4032-9131-744f88b09029"},"source":["# Wir machen google-drive dem Notebook bekannt, um Zugriff auf die Daten zu erhalten\n","from google.colab import drive\n","drive.mount('/content/drive')\n","base_dir = '/content/drive/MyDrive/hackerschool'"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K-LfIttMTdbi","executionInfo":{"status":"ok","timestamp":1615152195070,"user_tz":-60,"elapsed":604,"user":{"displayName":"Lena Herrmann","photoUrl":"","userId":"14614092248171651689"}}},"source":["def Detect(webcam_frame):\n","\n","    # Der aktuelle Frame der WebCam wird aus dem Parameter gelesen und in ein Numpy-Array übertragen \n","    binary = base64.b64decode(webcam_frame.split(',')[1])            \n","    frame = cv.imdecode(np.frombuffer(binary, np.uint8), -1)\n","    \n","    # Da in Opencv mit BGR-Bilder gearbeitet wird, muss das Bild nach RGB übertragen werden\n","    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n","\n","    # Hier wird die Kaskade zur Erkennung von frontalen Gesichtern geladen \n","    face_cascade = cv.CascadeClassifier()\n","    face_cascade.load(os.path.join(base_dir,'data/haarcascade_frontalface_alt.xml'))\n","    \n","    # Hier wird die Kaskade zur Erkennung von Augen geladen \n","    eye_cascade = cv.CascadeClassifier()\n","    eye_cascade.load(os.path.join(base_dir, 'data/haarcascade_eye.xml'))\n","    \n","    # Hier wird die Kaskade zur Erkennung von Mündern geladen \n","    # Diesen Teil sollt ihr gleich selbst implementieren\n","    # Die Datei die ihr braucht heißt: data/haarcascade_smile.xml\n","\n","    # Das RGB-Bild muss für den Algorithmus noch zu einem Schwarz/Weiß-Bild konvertiert werden\n","    frame_gray = cv.cvtColor(frame, cv.COLOR_RGB2GRAY)\n","    frame_gray = cv.equalizeHist(frame_gray)\n","    \n","    faces = face_cascade.detectMultiScale(frame_gray)\n","\n","    # Es wird über die Koordinaten von den erkannten Gesichtern iteriert\n","    for (x,y,w,h) in faces:\n","        # Hier wird ein Rechteck in Grün um das Gesicht gezeichnet\n","        cv.rectangle(frame, (x,y), (x+w, y+h), (0, 255,0), 2)\n","\n","        # Es wird eine Region of Interest (roi) im grauen und im bunten Bild gesetzt\n","        # Damit wird der zu betrachtene Ausschnitt auf das Gesicht reduziert, denn in \n","        # diesem Bereich vermuten wir weitere Bestandteile des Gesichts. \n","        roi_gray = frame_gray[y:y+h, x:x+w]        \n","        roi_color = frame[y:y+h, x:x+w]\n","        \n","        # Im grauen ROI wird nach den Augen gesucht\n","        eyes = eye_cascade.detectMultiScale(roi_gray)\n","        \n","        # Die Augen werden auf dem bunten Bild mit einem Rechteck umrahmt. \n","        for (ex,ey,ew,eh) in eyes:\n","            cv.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0, 0,255), 2)\n","\n","        # Fügt ab hier den Source-Code dazu, um auch den Mund im Gesicht zu\n","        # erkennen und diesen mit einem Rechteck zu umrahmen.\n","        # Schaut euch dazu nochmal an, wie die Augen erkannt wurden. \n","        \n","    return compress_to_bytes(frame)\n","\n","output.register_callback('notebook.Detect', Detect)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wk7dFWgwTjSz","executionInfo":{"status":"ok","timestamp":1615152198552,"user_tz":-60,"elapsed":589,"user":{"displayName":"Lena Herrmann","photoUrl":"","userId":"14614092248171651689"}}},"source":["def compress_to_bytes(data, fmt='png'):\n","    \"\"\"\n","    In dieser Hilfsfunktion wird das Bild in Bytes tranformiert und anschließend\n","    als in ein JSON-Objekt übertragen, damit das Bild and die JavaScript-Funktion \n","    zurück gesendet werden kann\n","    \"\"\"\n","    buff = io.BytesIO()\n","    img = PIL.Image.fromarray(data)    \n","    img.save(buff, format=fmt)\n","    \n","    data_encode = base64.b64encode(buff.getvalue())\n","    \n","    return IPython.display.JSON({'result': data_encode.decode(encoding='utf-8')})"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMx8ufWATkXH","executionInfo":{"status":"ok","timestamp":1615152201276,"user_tz":-60,"elapsed":553,"user":{"displayName":"Lena Herrmann","photoUrl":"","userId":"14614092248171651689"}}},"source":["def start_webcam():\n","\n","  js = Javascript('''\n","      (async function() {\n","\n","            // Wir erstellen Html-Elemente, welche den Input der WebCam zeigen\n","            var video = document.createElement('video');        \n","            video.id = 'video_stream'\n","            video.style.display = 'hidden';\n","            \n","            var div = document.createElement('div');\n","            div.id = 'video_container';   \n","\n","            // Mit diesem Button können wir die WebCam wieder ausschalten\n","            stop_button = document.createElement('button');\n","            stop_button.id = 'stop_button';\n","            stop_button.innerHTML = 'Stop Video';     \n","\n","            div.appendChild(video);\n","            div.appendChild(stop_button);\n","                          \n","            document.body.appendChild(div);\n","\n","            // Es wird die WebCam abgerufen, damit diese benutzt werden kann\n","            const stream = await navigator.mediaDevices.getUserMedia({video: true});      \n","            video.srcObject = stream;\n","\n","            // Es wir darauf gewartet, dass die WebCam einsatzbereit ist\n","            await video.play(); \n","            \n","            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","            \n","            // Das Video der WebCam wird auf eine Canvas gezeichnet\n","            var canvas = document.createElement('canvas');\n","            canvas.id = 'video_canvas';\n","            canvas.width = video.videoWidth;\n","            canvas.height = video.videoHeight;\n","\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            \n","            // Jede Sekunde wird die Detect-Methode aufgerufen, um das Gesicht zu erkennen.\n","            let triggerID = setInterval(async function call_detect() {\n","                \n","                canvas.getContext('2d').drawImage(video, 0, 0);             \n","                content = canvas.toDataURL('image/jpeg', 0.8);\n","\n","                const result = await google.colab.kernel.invokeFunction(\n","                    'notebook.Detect', // Name der Methode.\n","                    [content], // Der aktuelle Frame der WebCam ist der Parameter für die Detect-Methode.\n","                    {}); // kwargs\n","\n","                // Das Resultat der Detect-Methode wird wieder ausgelesen\n","                img_url = result.data['application/json']['result'];\n","                \n","                // Um das Ergebnis der Detect-Methode anzuzeigen, erstellen wir ein neues Image\n","                detected_image = new Image();\n","                detected_image.onload = function () {{\n","\n","                    var image_canvas = document.getElementById('image_canvas');\n","\n","                    if (image_canvas == null) {                          \n","                        image_canvas = document.createElement('canvas');\n","                        image_canvas.id = 'image_canvas';                  \n","                        document.body.appendChild(image_canvas)\n","                    }      \n","                    image_canvas.width = detected_image.width;\n","                    image_canvas.style.width = detected_image.width + 'px';\n","\n","                    image_canvas.height = detected_image.height;\n","                    image_canvas.style.height = detected_image.height + 'px';\n","                    \n","                    image_canvas.style.display = 'block';\n","                    \n","                    // das Bild wird auf die neue Canvas gezeichnet\n","                    image_canvas.getContext('2d').drawImage(detected_image, 0, 0);\n","\n","                  }};\n","\n","                  // Das Resultat der Detect-Methode ist das Bild mit erkannten Gesicht im Byte-Format.\n","                  // Dieses Byte-Array wird als Quelle für das Bild angegeben, damit es dargestellt werden kann.\n","                  detected_image.src = 'data:image/png;charset=utf-8;base64,'+img_url;\n","              \n","              } , 1000);\n","\n","            // Sobald der Button geklickt wird, wird die WebCam ausgeschaltet und die Detect-Methode\n","            // nicht mehr aufgerufen. \n","            stop_button.onclick = function() {                  \n","                  stream.getVideoTracks()[0].stop();\n","                  clearTimeout(triggerID);\n","          };\n","\n","      })();\n","    ''')\n","\n","  display.display(js)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":985},"id":"mU0SZGkIToer","executionInfo":{"status":"ok","timestamp":1615152233898,"user_tz":-60,"elapsed":531,"user":{"displayName":"Lena Herrmann","photoUrl":"","userId":"14614092248171651689"}},"outputId":"0b9e5153-6f69-4d6b-b64c-50afcd4e0a33"},"source":["# Mit diesem Methoden-Aufruf starten wir die WebCam.\n","start_webcam()"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","      (async function() {\n","\n","            // Wir erstellen Html-Elemente, welche den Input der WebCam zeigen\n","            var video = document.createElement('video');        \n","            video.id = 'video_stream'\n","            video.style.display = 'hidden';\n","            \n","            var div = document.createElement('div');\n","            div.id = 'video_container';   \n","\n","            // Mit diesem Button können wir die WebCam wieder ausschalten\n","            stop_button = document.createElement('button');\n","            stop_button.id = 'stop_button';\n","            stop_button.innerHTML = 'Stop Video';     \n","\n","            div.appendChild(video);\n","            div.appendChild(stop_button);\n","                          \n","            document.body.appendChild(div);\n","\n","            // Es wird die WebCam abgerufen, damit diese benutzt werden kann\n","            const stream = await navigator.mediaDevices.getUserMedia({video: true});      \n","            video.srcObject = stream;\n","\n","            // Es wir darauf gewartet, dass die WebCam einsatzbereit ist\n","            await video.play(); \n","            \n","            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","            \n","            // Das Video der WebCam wird auf eine Canvas gezeichnet\n","            var canvas = document.createElement('canvas');\n","            canvas.id = 'video_canvas';\n","            canvas.width = video.videoWidth;\n","            canvas.height = video.videoHeight;\n","\n","            canvas.getContext('2d').drawImage(video, 0, 0);\n","            \n","            // Jede Sekunde wird die Detect-Methode aufgerufen, um das Gesicht zu erkennen.\n","            let triggerID = setInterval(async function call_detect() {\n","                \n","                canvas.getContext('2d').drawImage(video, 0, 0);             \n","                content = canvas.toDataURL('image/jpeg', 0.8);\n","\n","                const result = await google.colab.kernel.invokeFunction(\n","                    'notebook.Detect', // Name der Methode.\n","                    [content], // Der aktuelle Frame der WebCam ist der Parameter für die Detect-Methode.\n","                    {}); // kwargs\n","\n","                // Das Resultat der Detect-Methode wird wieder ausgelesen\n","                img_url = result.data['application/json']['result'];\n","                \n","                // Um das Ergebnis der Detect-Methode anzuzeigen, erstellen wir ein neues Image\n","                detected_image = new Image();\n","                detected_image.onload = function () {{\n","\n","                    var image_canvas = document.getElementById('image_canvas');\n","\n","                    if (image_canvas == null) {                          \n","                        image_canvas = document.createElement('canvas');\n","                        image_canvas.id = 'image_canvas';                  \n","                        document.body.appendChild(image_canvas)\n","                    }      \n","                    image_canvas.width = detected_image.width;\n","                    image_canvas.style.width = detected_image.width + 'px';\n","\n","                    image_canvas.height = detected_image.height;\n","                    image_canvas.style.height = detected_image.height + 'px';\n","                    \n","                    image_canvas.style.display = 'block';\n","                    \n","                    // das Bild wird auf die neue Canvas gezeichnet\n","                    image_canvas.getContext('2d').drawImage(detected_image, 0, 0);\n","\n","                  }};\n","\n","                  // Das Resultat der Detect-Methode ist das Bild mit erkannten Gesicht im Byte-Format.\n","                  // Dieses Byte-Array wird als Quelle für das Bild angegeben, damit es dargestellt werden kann.\n","                  detected_image.src = 'data:image/png;charset=utf-8;base64,'+img_url;\n","              \n","              } , 1000);\n","\n","            // Sobald der Button geklickt wird, wird die WebCam ausgeschaltet und die Detect-Methode\n","            // nicht mehr aufgerufen. \n","            stop_button.onclick = function() {                  \n","                  stream.getVideoTracks()[0].stop();\n","                  clearTimeout(triggerID);\n","          };\n","\n","      })();\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}